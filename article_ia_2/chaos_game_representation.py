# -*- coding: utf-8 -*-
"""Chaos_Game_Representation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15axaBtuEwtjGClb7Q-q7qxTsXJskMLhp

## Load libraries
"""

from collections import defaultdict
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

from sklearn.model_selection import train_test_split

import tensorflow as tf
import keras
from tensorflow.keras.layers import Conv2D, Dense, Dropout, BatchNormalization, MaxPool2D
from tensorflow.keras.models import load_model
from keras.utils import to_categorical

# SVM part

from sklearn.datasets import make_classification
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

from sklearn.metrics import ConfusionMatrixDisplay

"""# Mount drive"""

from google.colab import drive
drive.mount('/content/drive')

"""# Load data"""

working_folder = "/content/drive/MyDrive/Sequences/"

human_sequences = "/content/drive/MyDrive/Sequences/human.txt"
dog_sequences = "/content/drive/MyDrive/Sequences/dog.txt"
chimpanze_sequences = "/content/drive/MyDrive/Sequences/chimpanzee.txt"

df_human = pd.read_csv(human_sequences, sep = "\t")
df_dog = pd.read_csv(dog_sequences, sep = "\t")
df_chimpanze = pd.read_csv(chimpanze_sequences, sep = "\t")


"""# Exploratory Data Analysis"""

labels = [1, 2, 3, 4, 5, 6]
plt.figure(figsize = (15, 10))
plt.subplot(131)
plt.pie(df_human["class"].value_counts(sort = False).sort_index() , autopct='%1.1f%%')
plt.title("Proportion of sequences classes in human")
plt.legend(title="Class",
           labels=labels,
          loc="lower right",)

plt.subplot(132)
labels_dog = df_dog["class"].unique()
plt.pie(df_dog["class"].value_counts(sort = False).sort_index() , autopct='%1.1f%%')
plt.title("Proportion of sequences classes in dog")
plt.legend(title="Class",
           labels=labels,
          loc="lower right",)

plt.subplot(133)
labels_chimpanze = df_chimpanze["class"].unique()
plt.pie(df_chimpanze["class"].value_counts(sort = False).sort_index() , autopct='%1.1f%%')
plt.title("Proportion of sequences classes in chimpanze")
plt.legend(title="Class",
           labels=labels,
          loc="lower right",)

plt.savefig(working_folder + "EDA.png")
plt.show()

"""### Add column for sequence length"""

df_human["sequence_length"] = len(df_human["sequence"])
df_human.head()

df_dog["sequence_length"] = len(df_dog["sequence"])
df_dog.head()

df_chimpanze["sequence_length"] = len(df_chimpanze["sequence"])
df_chimpanze.head()

"""## Sanitize data

### Human
"""

## Delete sequences that contains N
nb_sequences_to_drop = 0
rows_indexes_to_drop = list()

for idx, seq in enumerate(df_human['sequence']):
    if 'N' in seq:
      nb_sequences_to_drop +=1
      # display(df.loc[df['sequence'] == seq])

      rows_indexes_to_drop.append(idx)

print(f"Number of sequence to be dropped : {nb_sequences_to_drop}")
print(f"Rows to be dropped : {rows_indexes_to_drop}")

print(f"Shape of the data before dropping sequences containing Ns : {df_human.shape}")

df_human.drop(rows_indexes_to_drop, inplace = True)
print(f"Shape of the data after dropping sequences containing Ns : {df_human.shape}")

"""### Dog"""

nb_sequences_to_drop = 0
rows_indexes_to_drop = list()

for idx, seq in enumerate(df_dog['sequence']):
    if 'N' in seq:
      nb_sequences_to_drop +=1
      rows_indexes_to_drop.append(idx)

print(f"Number of sequence to be dropped : {nb_sequences_to_drop}")
print(f"Rows to be dropped : {rows_indexes_to_drop}")

print(f"Shape of the data before dropping sequences containing Ns : {df_dog.shape}")

df_dog = df_dog.drop(rows_indexes_to_drop, inplace = True)

"""### Chimpanze"""

nb_sequences_to_drop = 0
rows_indexes_to_drop = list()

for idx, seq in enumerate(df_chimpanze['sequence']):
    if 'N' in seq:
      nb_sequences_to_drop +=1
      rows_indexes_to_drop.append(idx)

print(f"Number of sequence to be dropped : {nb_sequences_to_drop}")
print(f"Rows to be dropped : {rows_indexes_to_drop}")

print(f"Shape of the data before dropping sequences containing Ns : {df_human.shape}")

df_chimpanze.drop(rows_indexes_to_drop, inplace = True)

"""## CGR transformation"""

def cgr_encoding(sequence, k):
  """
  sequence : Sequence to encode as CGR
  k = k-mers size
  """

  kmer_count = defaultdict(int)
  for i in range(len(sequence)-(k-1)):

      kmer_count[str(sequence[i:i+k])] +=1
  for key in kmer_count.keys():
      if "N" in key:
          del kmer_count[key]

  probabilities = defaultdict(float)
  N = len(sequence)
  for key, value in kmer_count.items():
      probabilities[key] = float(value) / (N - k + 1)


  array_size = int(np.sqrt(4**k))
  chaos = []
  for i in range(array_size):
      chaos.append([0]*array_size)

  maxx = array_size
  maxy = array_size
  posx = 1
  posy = 1
  for key, value in probabilities.items():
      for char in key:
          if char == "T":
              posx += maxx / 2
          elif char == "C":
              posy += maxy / 2
          elif char == "G":
              posx += maxx / 2
              posy += maxy / 2
          maxx = maxx / 2
          maxy /= 2

      chaos[int(posy)-1][int(posx)-1] = value
      maxx = array_size
      maxy = array_size
      posx = 1
      posy = 1

  return tf.constant(chaos)

"""## Plot data"""

# Randomly selected one example per class
random_idx_class_0 = np.random.choice(df_human[df_human["class"] == 0].index)
random_idx_class_1 = np.random.choice(df_human[df_human["class"] == 1].index)
random_idx_class_2 = np.random.choice(df_human[df_human["class"] == 2].index)
random_idx_class_3 = np.random.choice(df_human[df_human["class"] == 3].index)
random_idx_class_4 = np.random.choice(df_human[df_human["class"] == 4].index)
random_idx_class_5 = np.random.choice(df_human[df_human["class"] == 5].index)

# Plot some data
plt.figure(figsize = (10, 10))

plt.subplot(231)
plt.imshow(cgr_encoding(sequence = df_human["sequence"][random_idx_class_0], k = 5), cmap = "gray")
plt.title("CGR of class 0")

plt.subplot(232)
plt.imshow(cgr_encoding(sequence = df_human["sequence"][random_idx_class_1], k = 5), cmap = "gray")
plt.title("CGR of class 1")

plt.subplot(233)
plt.imshow(cgr_encoding(sequence = df_human["sequence"][random_idx_class_2], k = 5), cmap = "gray")
plt.title("CGR of class 2")

plt.subplot(234)
plt.imshow(cgr_encoding(sequence = df_human["sequence"][random_idx_class_3], k = 5), cmap = "gray")
plt.title("CGR of class 3")

plt.subplot(235)
plt.imshow(cgr_encoding(sequence = df_human["sequence"][random_idx_class_4], k = 5), cmap = "gray")
plt.title("CGR of class 4")

plt.subplot(236)
plt.imshow(cgr_encoding(sequence = df_human["sequence"][random_idx_class_5], k = 5), cmap = "gray")
plt.title("CGR of class 5")

plt.subplots_adjust(wspace=0.12, hspace=0)
plt.show()

"""## Learning part

### Generate data
"""

encoded_list = []

for seq in df_human["sequence"]:
  encoded_list.append(cgr_encoding(seq, k =5))

X, y  = np.array(encoded_list), to_categorical(df_human["class"])

"""### Data spliting"""

X = X.reshape(4020, 32, 32, 1)

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y, shuffle = True, test_size = 0.2) # Split data into train and test with respect of class proportions

X_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test, random_state = 42, stratify = y_test, shuffle = True, test_size = 0.5) # Split data into test and validation with respect of class proportions

model = tf.keras.Sequential()

model.add(Conv2D(filters = 32, kernel_size = 3, activation = 'relu', padding = "valid", input_shape = (32, 32, 1), name = "Conv_1")) # Input shape : Batch_size, width, height, channels
model.add(Dropout(0.3, name = "Dropout_1"))
model.add(MaxPool2D(pool_size=4, strides=None, padding='valid', name = "MaxPool_1"))
model.add(Conv2D(filters = 64, kernel_size = 2, activation = 'relu', padding = "valid", name = "Conv_2")) # Input shape : Batch_size, width, height, channels
model.add(Dropout(0.3, name = "Dropout_2"))
model.add(tf.keras.layers.Flatten(name = "Flatten"))
model.add(tf.keras.layers.Dense(units = 7, activation = 'softmax', name = "Output"))

model.summary()

# Compile model
model.compile(loss='categorical_crossentropy',
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              metrics=['accuracy'])

# Define early stopping to avoid overfitting

early_stop = keras.callbacks.EarlyStopping(monitor = 'val_accuracy', min_delta = 0.0005, patience=8,
                                           restore_best_weights=True )

# Learning from data
history = model.fit(X_train, y_train, batch_size = 8, validation_data= (X_test, y_test),
                        epochs=200)

"""## Plot results"""

plt.figure(figsize = (10,4))
plt.subplot(121)
plt.plot(history.epoch, history.history["loss"], 'g', label='Training loss')
plt.plot(history.epoch, history.history["val_loss"], 'b', label='Validation loss')
plt.title('Training loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.subplot(122)
plt.plot(history.epoch, history.history["accuracy"], 'r', label='Training accuracy')
plt.plot(history.epoch, history.history["val_accuracy"], 'orange', label='Validation accuracy')
plt.title('Training accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.savefig(working_folder + "base_modelpng")
plt.show()

"""### Save model"""

working_folder = "/content/drive/MyDrive/Sequences/"
model.save(working_folder)

"""### Model evaluation"""

loss, acc = model.evaluate(X_validation, y_validation, verbose=2)
print("Basic model, accuracy: {:5.2f}%".format(100 * acc))

"""## Load LeNet5"""

lenet_folder = "/content/drive/MyDrive/LeNet/"

lenet_model = load_model(lenet_folder)

"""### Check LeNet model"""

lenet_model.summary()

"""### Freeze bottom layers for transfer learning"""

# Freeze the first layers
for idx, layer in enumerate(lenet_model.layers):
  if idx <= 1:
    layer.trainable = False

"""### Pop last dense layer and replace it for relevent goal"""

# Remove last layer
lenet_model.pop()

# Replace it with a more appropriate
lenet_model.add(tf.keras.layers.Dense(units = 7, activation = 'softmax', name = "Output"))

"""### Check modified model"""

lenet_model.summary()

"""### Compile model and train"""

lenet_model.compile(loss='categorical_crossentropy',
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              metrics=['accuracy'])

# Define early stopping to avoid overfitting
early_stop = keras.callbacks.EarlyStopping(monitor = 'val_accuracy', min_delta = 0.0005, patience=8,
                                           restore_best_weights=True )

history_lenet = lenet_model.fit(X_train, y_train, batch_size = 8, validation_data= (X_test, y_test),
                        epochs=200)

transfer_learning_directory = "/content/drive/MyDrive/Sequences/transfert_learning/"


lenet_model.save(transfer_learning_directory)

"""# Plot final results"""

plt.figure(figsize = (14,4))
plt.subplot(121)
plt.plot(history.epoch, history.history["loss"], 'g', linestyle = 'dotted', label='Training - LeNet')
plt.plot(history_lenet.epoch, history_lenet.history["loss"], 'g', label='Training - LeNet')
plt.plot(history.epoch, history.history["val_loss"], 'b', label='Validation loss')
plt.plot(history_lenet.epoch, history_lenet.history["val_loss"], 'b' , linestyle = 'dotted', label='Validation  - LeNet')
plt.title('Training loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.subplot(122)
plt.plot(history.epoch, history.history["accuracy"], 'r', label='Training accuracy')
plt.plot(history_lenet.epoch, history_lenet.history["accuracy"], 'r',linestyle = 'dotted', label='Training - LeNet')

plt.plot(history.epoch, history.history["val_accuracy"], 'orange', label='Validation accuracy')
plt.plot(history_lenet.epoch, history_lenet.history["val_accuracy"], 'orange', linestyle = 'dotted', label='Validation - LeNet')

plt.title('Training accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.savefig(transfer_learning_directory + "dna_lenet_model.png")
plt.show()

"""### Evaluate model"""

loss, acc = lenet_model.evaluate(X_validation, y_validation, verbose=2)
print("DNA LeNet model, accuracy: {:5.2f}%".format(100 * acc))

"""## Change classification layer for SVM

### Adapt intial CNN for SVM classification
"""

# Chop off head of CNN

modified_model = model
modified_model.pop()
modified_model.summary()

# Go back on labels from one hot encoding
X_train_svm  = modified_model.predict(X_train)
y_train_svm = np.argmax(y_train, axis=1)

X_test_svm = modified_model.predict(X_test)
y_test_svm = np.argmax(y_test, axis=1)

X_validation_svm = modified_model.predict(X_validation)
y_validation_svm = np.argmax(y_validation, axis=1)

"""### Prepare data for svm"""


"""### Checking dimensions"""

print(f"Shape of x_train_svm : {X_train_svm.shape}")
print(f"Shape of y_train_svm : {y_train_svm.shape}")

print(f"Shape of x_test_svm : {X_test_svm.shape}")
print(f"Shape of y_test_svm : {y_test_svm.shape}")

print(f"Shape of x_validation_svm : {X_validation_svm.shape}")
print(f"Shape of y_validation_svm : {y_validation_svm.shape}")

"""### Instanciate SVM classifier and train it"""

SVM_classifier = SVC(decision_function_shape = 'ovo', class_weight = "balanced", C = 125.0)

SVM_classifier.fit(X_train_svm, y_train_svm, )

"""### Evaluate classification"""

# Make predictions on test set

y_pred_svm = SVM_classifier.predict(X_test_svm)

accuracy = accuracy_score(y_test_svm, y_pred_svm)

print(f"Acuracy using CNN coupled to SVM : {accuracy}")

"""## Confusion matrix"""

class_names = np.arange(7)

# Plot non-normalized confusion matrix
titles_options = [
    ("Confusion matrix, without normalization", None),
    ("Normalized confusion matrix", "true"),
]
for title, normalize in titles_options:
    disp = ConfusionMatrixDisplay.from_estimator(
        SVM_classifier,
        X_test_svm,
        y_test_svm,
        display_labels=class_names,
        cmap=plt.cm.Blues,
        normalize=normalize,
    )
    disp.ax_.set_title(title)

    print(title)
    print(disp.confusion_matrix)

plt.savefig(working_folder + "svm_confusion_matrix.png")
plt.show()


